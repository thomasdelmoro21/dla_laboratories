{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac4a402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34f171ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"',-.:;<>?ABCDEFGHILMNOPQRSTUVXZ`abcdefghilmnopqrstuvxz\n",
      "Vocab Size: 58\n"
     ]
    }
   ],
   "source": [
    "with open('dante.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab Size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31f64d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 40, 45, 45, 48]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# create a mapping of characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))\n",
    "\n",
    "# Using character level encoding we obtain very long sequences of integers for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa8112e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(len(data) * 0.9)\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73f83804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 13,  1, 16, 21, 32, 21, 24, 13])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ee19c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([22]) the target is 13\n",
      "When input is tensor([22, 13]) the target is 1\n",
      "When input is tensor([22, 13,  1]) the target is 16\n",
      "When input is tensor([22, 13,  1, 16]) the target is 21\n",
      "When input is tensor([22, 13,  1, 16, 21]) the target is 32\n",
      "When input is tensor([22, 13,  1, 16, 21, 32]) the target is 21\n",
      "When input is tensor([22, 13,  1, 16, 21, 32, 21]) the target is 24\n",
      "When input is tensor([22, 13,  1, 16, 21, 32, 21, 24]) the target is 13\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc16ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[52, 40, 42, 54, 40,  0,  1,  1],\n",
      "        [36, 47, 57, 36,  1, 51, 44, 47],\n",
      "        [ 1, 38, 43,  4, 40, 51, 36,  1],\n",
      "        [ 1, 10, 10, 25,  1, 36, 47, 44]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[40, 42, 54, 40,  0,  1,  1, 52],\n",
      "        [47, 57, 36,  1, 51, 44, 47, 42],\n",
      "        [38, 43,  4, 40, 51, 36,  1, 49],\n",
      "        [10, 10, 25,  1, 36, 47, 44, 46]])\n",
      "----\n",
      "When input is tensor([52]) the target is 40\n",
      "When input is tensor([52, 40]) the target is 42\n",
      "When input is tensor([52, 40, 42]) the target is 54\n",
      "When input is tensor([52, 40, 42, 54]) the target is 40\n",
      "When input is tensor([52, 40, 42, 54, 40]) the target is 0\n",
      "When input is tensor([52, 40, 42, 54, 40,  0]) the target is 1\n",
      "When input is tensor([52, 40, 42, 54, 40,  0,  1]) the target is 1\n",
      "When input is tensor([52, 40, 42, 54, 40,  0,  1,  1]) the target is 52\n",
      "When input is tensor([36]) the target is 47\n",
      "When input is tensor([36, 47]) the target is 57\n",
      "When input is tensor([36, 47, 57]) the target is 36\n",
      "When input is tensor([36, 47, 57, 36]) the target is 1\n",
      "When input is tensor([36, 47, 57, 36,  1]) the target is 51\n",
      "When input is tensor([36, 47, 57, 36,  1, 51]) the target is 44\n",
      "When input is tensor([36, 47, 57, 36,  1, 51, 44]) the target is 47\n",
      "When input is tensor([36, 47, 57, 36,  1, 51, 44, 47]) the target is 42\n",
      "When input is tensor([1]) the target is 38\n",
      "When input is tensor([ 1, 38]) the target is 43\n",
      "When input is tensor([ 1, 38, 43]) the target is 4\n",
      "When input is tensor([ 1, 38, 43,  4]) the target is 40\n",
      "When input is tensor([ 1, 38, 43,  4, 40]) the target is 51\n",
      "When input is tensor([ 1, 38, 43,  4, 40, 51]) the target is 36\n",
      "When input is tensor([ 1, 38, 43,  4, 40, 51, 36]) the target is 1\n",
      "When input is tensor([ 1, 38, 43,  4, 40, 51, 36,  1]) the target is 49\n",
      "When input is tensor([1]) the target is 10\n",
      "When input is tensor([ 1, 10]) the target is 10\n",
      "When input is tensor([ 1, 10, 10]) the target is 25\n",
      "When input is tensor([ 1, 10, 10, 25]) the target is 1\n",
      "When input is tensor([ 1, 10, 10, 25,  1]) the target is 36\n",
      "When input is tensor([ 1, 10, 10, 25,  1, 36]) the target is 47\n",
      "When input is tensor([ 1, 10, 10, 25,  1, 36, 47]) the target is 44\n",
      "When input is tensor([ 1, 10, 10, 25,  1, 36, 47, 44]) the target is 46\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(110)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "n_embed = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb , yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f50d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 58])\n",
      "tensor(4.1255, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "Cell \u001b[1;32mIn[78], line 30\u001b[0m, in \u001b[0;36mBigramLanguageModel.generate\u001b[1;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, max_new_tokens):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m---> 30\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# keep only the last time step\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[78], line 14\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# idx and target are both (B,T) tensor of integers\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(idx) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(tok_emb) \u001b[38;5;66;03m# (B,T,V)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and target are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        logits = self.lm_head(tok_emb) # (B,T,V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # keep only the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size, n_embed).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(torch.zeros(1, 1).long(), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa2554bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1dfef86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4525880813598633\n",
      "2.3348538875579834\n",
      "2.413069009780884\n",
      "2.5114784240722656\n",
      "2.5023553371429443\n",
      "2.4476187229156494\n",
      "2.437863349914551\n",
      "2.4656219482421875\n",
      "2.4230082035064697\n",
      "2.5302159786224365\n",
      "2.396296739578247\n",
      "2.4570095539093018\n",
      "2.4547176361083984\n",
      "2.375426769256592\n",
      "2.365833044052124\n",
      "2.3515896797180176\n",
      "2.3716814517974854\n",
      "2.266244649887085\n",
      "2.446502447128296\n",
      "2.2659053802490234\n",
      "2.418915271759033\n",
      "2.423917293548584\n",
      "2.49324107170105\n",
      "2.2993669509887695\n",
      "2.531456470489502\n",
      "2.301499128341675\n",
      "2.4209437370300293\n",
      "2.5618040561676025\n",
      "2.441148042678833\n",
      "2.3815388679504395\n",
      "2.4020280838012695\n",
      "2.442016124725342\n",
      "2.414278030395508\n",
      "2.4093053340911865\n",
      "2.34110689163208\n",
      "2.3584768772125244\n",
      "2.436560869216919\n",
      "2.4152917861938477\n",
      "2.2931642532348633\n",
      "2.346449613571167\n",
      "2.3698947429656982\n",
      "2.462114095687866\n",
      "2.3524081707000732\n",
      "2.2900230884552\n",
      "2.400932550430298\n",
      "2.542382001876831\n",
      "2.4307029247283936\n",
      "2.384967803955078\n",
      "2.4911422729492188\n",
      "2.476912498474121\n",
      "2.4833834171295166\n",
      "2.3555665016174316\n",
      "2.3641014099121094\n",
      "2.3823211193084717\n",
      "2.5261852741241455\n",
      "2.4929397106170654\n",
      "2.4654598236083984\n",
      "2.361354112625122\n",
      "2.4280455112457275\n",
      "2.3758273124694824\n",
      "2.565467596054077\n",
      "2.4320952892303467\n",
      "2.4956889152526855\n",
      "2.47786021232605\n",
      "2.462723731994629\n",
      "2.4337027072906494\n",
      "2.3382515907287598\n",
      "2.4477808475494385\n",
      "2.2387819290161133\n",
      "2.388500928878784\n",
      "2.3612022399902344\n",
      "2.414313316345215\n",
      "2.4934134483337402\n",
      "2.318331003189087\n",
      "2.3443915843963623\n",
      "2.46396541595459\n",
      "2.438295841217041\n",
      "2.4337005615234375\n",
      "2.38813853263855\n",
      "2.3675272464752197\n",
      "2.4151980876922607\n",
      "2.398313283920288\n",
      "2.350792407989502\n",
      "2.31177020072937\n",
      "2.296771764755249\n",
      "2.455925703048706\n",
      "2.505807638168335\n",
      "2.4399397373199463\n",
      "2.3799169063568115\n",
      "2.3947882652282715\n",
      "2.415220022201538\n",
      "2.3709816932678223\n",
      "2.3775758743286133\n",
      "2.3529725074768066\n",
      "2.501095771789551\n",
      "2.3844454288482666\n",
      "2.5047261714935303\n",
      "2.283088207244873\n",
      "2.4439477920532227\n",
      "2.43232798576355\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66a1fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "roroli ovi,\n",
      "\n",
      " pedoltrine o` chia,\n",
      "\n",
      "  fa vrossscopRquatore con doco ttrostria posctosave SLL a l',\n",
      "\n",
      "\n",
      " son cledi ctatolo l'l ciomalli conistor la tar 'i  ma:\n",
      " roCia\n",
      " ce essstoederntoli'uar stialeml che datarsi>.\n",
      "\n",
      " pina mX.\n",
      "\n",
      "\n",
      "\n",
      " so\n",
      "\n",
      "\n",
      " npp fe,min gespi catsse p r pmpe tallol len'ave.\n",
      " n iste  qpnol'indire cal m'i;:mita goDe;\n",
      "  tisici` va di 'iZH<<QZ hco la ` chenosufi lcchero  ma adibMal  aAchegn co s'\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros(1, 1).long(), max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107e4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy)\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
