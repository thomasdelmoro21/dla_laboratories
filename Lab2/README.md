# Lab. 2 - LLMs
This laboratory is focused on the use of Language Models (LMs) and Large Language Models (LLMs) and their applications. It covers varius NLP tasks shuch as text generation, text classification and question answering. We use the `HuggingFace` library to load datasets and pre-trained models for our tasks. 

## Introduction
- `Lab1.ipynb` constains the notebook with complete code of the solved exercises (they are not shown all the results).
- `./results` is the directory where they are saved all the resulting outputs displayed in this report.  
- In this lab the trained models are not updated in the repository beacause of their bigger size.
- This report contains the informations about how the exercises are solved and it shows the most significant results.

## Exercise 1 - Small GPT model
In this first exercise we implement a small autoregressive GPT model for character generation to generate text in the style of Dante Alighieri.  
The code is inspired by the Andrej Karpathy [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) and [correlated repository](https://github.com/karpathy/ng-video-lecture), while as input data is was used a `.txt` file containing the Inferno by Dante Alighieri.  
It is shown below a frame of the generated text in the style of Dante.

```
Carisse la barbana chi in sannegno,
  di se' hno Cacanto Ausai,
  anon dinanzi fine costarina,

te ne la levene che sti` naffello;
  fu d'ivirebbi  tremi e la tra sermina
  dal de lidinanto e vetanto e quante.

<<Brancome che que,non pur nel malto,
  mi si`, ed antato convino rameva:
  ancor aroppia, cosi` ch'era' Ore espiri.

Poi gia` a quella coda sola l'aniva
  loco coglicar del ciglio che 'l fruttatimo,
  e di quante men far molte anime>>.
```
As we can observe, the structure in triplets is almost everywhere maintained but some words don't exist in today's Italian language nor in that of Dante's time.

## Exercise 2 - Large GPT model
In this simple exercise we delve into the use of `GPT2Tokenizer` and `GPT2LMHeadModel`.  
Firstly, we load the GPT2Tokenizer and we try to tokenize the previous Dante's Inferno text, so we can compare the length difference between the original text and the tokenized one.  
Secondly, we load the GPT2LMHeadModel and we try to generate text startinge from a prompt.  

### Tokenization
They are show below the results of the tokenization process using `GPT2Tokenizer`. 

<div align="center">

| | Number of tokens (characters)| Ratio |
| ------------------ |:-----------:|:------:|
| Original text | 186 983 |
| Tokenized text | 79 225 |
| Ratio | | **42.4%** |

</div>

### Text generation
We start from the short prompt "*I am*" and we show the first 100 tokens generated by the `GPT2LMHeadModel`.  
The default inference mode for GPT2 is *greedy* which not results in satisfying generated text. So, we look at the values of `do_sample` and `temperature` parameters. Specifically:
- `do_sample = True` allows the model to sample the next token from the distribution of the next token.
- `temperature` is a hyperparameter that controls the randomness of the next token. The higher the temperature, the more random the next token.

We generate text with several values of these paratemers and we show the results below.  
As we can observe, the text generated with `do_sample = False` is very repetitive due to the greedy approach. On the contrary, with `do_sample = True` and a high level of temperature the randomness of the text is very high so we can obtain very different results.
Ideally, the best results are should be obtained with sampling enabled and a reasonable temperature value.

```
**do_sample = False**

I am not a fan of the idea of a "big-budget" movie. I think it's a waste of money. I think it's a waste of time. I think it's a waste of money. I think it's a waste of time. I think it's a waste of time. I think it's a waste of time. I think it's a waste of time. I think it's a waste of time. I think it's a waste of time. I think it's


**do_sample = True, temperature = 0.1**

I am not sure if this is a good idea or not, but I am sure that it will be a good idea.  
I am not sure if this is a good idea or not, but I am sure that it will be a good idea. I am not sure if this is a good idea or not, but I am sure that it will be a good idea. I am not sure if this is a good idea or not, but I am sure that it will be a good idea.


**do_sample = True, temperature = 0.5**

I am very proud of the work I have done and am looking forward to the next chapter in my life."  
The former White House aide, who is now a senior adviser to the president, is one of several former aides who have been named to the post.  
Among them: former national security adviser Michael Flynn, former CIA director Mike Pompeo and former White House chief of staff Reince Priebus.  
The Post reported that Flynn and Priebus were named to the post in May, after a


**do_sample = True, temperature = 0.9**

I am not sure of the timing," says Dr. Alvaro. "We will monitor the results of the study again at another clinic, but I am not confident it will be done right because of the risks that the study will bring us."  
For more information on the upcoming research, please contact:  
Dr. Alvaro S. de Araujo, MD  
M.D.  
Dr. Sato della A. de Araujo, M.D  
```

## Exercise 3.1 - Text Classification
Here, we leverage DistillBERT to execute a classification task. We load the pre-trained model, adding a classification head on top of it and test it on two different datasets.
- [IMDB](https://huggingface.co/datasets/ajaykarthick/imdb-movie-reviews): we take a small subset of the IMDB dataset for binary sentiment classification.
- [Tweet Eval](https://huggingface.co/datasets/cardiffnlp/tweet_eval): a small dataset of tweets for multi-class classification (4 classes).

The performance are measured in three different cases:
- `Zero-Shot`: we add a classification head initialized at random and we test the model without any training process.
- `Training the Classification Head`: we freeze the DistillBERT backbone and we train only the added classification head.
- `Fine-Tuning`: we fine-tune the entire model with very low learning rate for the backbone and higher learning rate for the classification head.

<div align="center">

| Dataset | Zero-Shot | Training the Classifier | Fine-Tuning |
| ------------------ |:-----------:|:------:|:------:|
| **IMDB** | 0.5110 | 0.8620 | **0.9090** |
| **Tweet Eval** | 0.3688 | 0.7431 | **0.7994** |
</div>
As we expected, the fine-tuning process is the most effective one, while the zero-shot seems to do almost random predictions.

## Exercise 3.2 - Question Answering Model
This exercise, that is a bit harder than the previous ones, consists of implementing a Multiple-Choice Question Answering model using DistillBERT.  
We use again the `HuggingFace` library to load the pre-trained model and to fine-tune it using the `Trainer` and `TrainingArguments` modules.  
We fine-tune the model on the [SWAG dataset](https://huggingface.co/datasets/allenai/swag) that consists of four possible choices for each sentence.  
After the training process, we qualitatively evaluate the performances asking some simple question to the model and observing its answers. They are show below some examples of the questions and the answers given by the model.

```
What is the capital of France?
0: Paris
1: London
2: Berlin
3: Madrid
Model answer: Paris

Who was the president of the United States?
0: Kobe Briant
1: Barack Obama
2: Robin Hood
3: Michael Jackson
Model answer: Barack Obama

Who is Matt Demon?
0: Tennis player
1: Singer
2: Lawyer
3: Actor
Model answer: Actor
```
